{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_7/2fdls1m56pg0353q2ntmqz7w0000gn/T/ipykernel_52068/3294665433.py:12: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('svg', 'pdf') # For export\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "from skimage.util import random_noise\n",
    "\n",
    "## PyTorch\n",
    "## Backprop Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "# Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "\n",
    "## Hebbian Learning\n",
    "from nupic.torch.modules import (\n",
    "    KWinners2d, KWinners, SparseWeights, SparseWeights2d, Flatten,\n",
    "    rezero_weights, update_boost_strength\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations applied on each image => only make them a tensor\n",
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                ])\n",
    "\n",
    "# Loading the training dataset. We need to split it into a training and validation part\n",
    "dataset = ImageFolder(\"../trafic_32\", transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "train_loader = data.DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Sparsity parameters\n",
    "SPARSITY = 0.66\n",
    "SPARSITY_CNN = SPARSITY\n",
    "\n",
    "# K-Winners parameters\n",
    "PERCENT_ON = 0.66\n",
    "BOOST_STRENGTH = 1.4\n",
    "relu_flag = True\n",
    "\n",
    "# Models\n",
    "latent_dim = 256\n",
    "img_size = 32 * 32 * 3\n",
    "\n",
    "# W-GAN PARAMETERS\n",
    "WEIGHT_CLIP = 0.01\n",
    "disc_iter = 5\n",
    "\n",
    "# Layers\n",
    "drop_out = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Cond_Generator(nn.Module):\n",
    "    def __init__(self, batch_size,latent_dim, hidden_dim, output_dim, num_of_classes,SPARSITY_CNN,BOOST_STRENGTH,PERCENT_ON,relu_flag):\n",
    "        super(Cond_Generator, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = int(output_dim)\n",
    "        self.num_of_classes = num_of_classes\n",
    "        self.relu_flag= relu_flag\n",
    "        self.SPARSITY_CNN = SPARSITY_CNN\n",
    "        #\n",
    "        # self.fc_1 = nn.Sequential(nn.Linear(self.latent_dim+self.num_of_classes, self.latent_dim+self.num_of_classes))\n",
    "        #\n",
    "        # # self.fc_2 = nn.Sequential(SparseWeights(nn.Linear(self.hidden_dim, self.hidden_dim), sparsity=SPARSITY),\n",
    "        # # KWinners(n=self.hidden_dim, percent_on=PERCENT_ON, boost_strength=BOOST_STRENGTH,relu=self.relu_flag),)\n",
    "        #\n",
    "        # self.fc_2 = nn.Sequential(nn.Linear(self.latent_dim+self.num_of_classes, int(self.hidden_dim)))\n",
    "        # self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.cntv_1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(int(self.latent_dim), int(self.hidden_dim*8), kernel_size=(3, 3), stride=(1, 1), padding=(0, 0), bias=False),\n",
    "            KWinners2d(channels=int(self.hidden_dim*8), percent_on=PERCENT_ON, boost_strength=BOOST_STRENGTH,relu=self.relu_flag),\n",
    "            nn.Dropout(drop_out)\n",
    "        )\n",
    "\n",
    "        self.cntv_2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d( int(self.hidden_dim*8), int(self.hidden_dim*4), kernel_size=(3,3), stride=(2,2), padding=(1,1), bias=False),\n",
    "            nn.BatchNorm2d(int(self.hidden_dim*4)),\n",
    "            KWinners2d(channels=int(self.hidden_dim*4), percent_on=PERCENT_ON, boost_strength=BOOST_STRENGTH,relu=self.relu_flag),\n",
    "            nn.Dropout(drop_out)\n",
    "        )\n",
    "\n",
    "        self.cntv_3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(int(self.hidden_dim*4), int(self.hidden_dim*2), kernel_size=(3,3), stride=(2,2), padding=(1,1), bias=False),\n",
    "            nn.BatchNorm2d(int(self.hidden_dim*2)),\n",
    "            KWinners2d(channels=int(self.hidden_dim*2), percent_on=PERCENT_ON, boost_strength=BOOST_STRENGTH,relu=self.relu_flag),\n",
    "            nn.Dropout(drop_out)\n",
    "        )\n",
    "\n",
    "        self.cntv_4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(int(self.hidden_dim*2), int(self.hidden_dim), kernel_size=(3,3), stride=(2,2), padding=(1,1), bias=False),\n",
    "            nn.BatchNorm2d(int(self.hidden_dim)),\n",
    "            KWinners2d(channels=int(self.hidden_dim), percent_on=PERCENT_ON, boost_strength=BOOST_STRENGTH,relu=self.relu_flag),\n",
    "            nn.Dropout(drop_out)\n",
    "        )\n",
    "\n",
    "        self.cntv_5 =  nn.Sequential(\n",
    "            nn.ConvTranspose2d(int(self.hidden_dim), int(self.hidden_dim/2), kernel_size=(3,3), stride=(2,2), padding=(1,1), bias=False),\n",
    "            nn.BatchNorm2d(int(self.hidden_dim/2)),\n",
    "            KWinners2d(channels=int(self.hidden_dim/2), percent_on=PERCENT_ON, boost_strength=BOOST_STRENGTH,relu=self.relu_flag),\n",
    "            nn.Dropout(drop_out)\n",
    "        )\n",
    "\n",
    "        self.cntv_6 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(int(self.hidden_dim/2), int(self.output_dim/32/32), kernel_size=(4,4), stride=(1,1), padding=(2,2), bias=False),\n",
    "        )\n",
    "        self.weights_init()\n",
    "    def weights_init(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "            torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "        if isinstance(m, nn.BatchNorm2d):\n",
    "            torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "            torch.nn.init.constant_(m.bias, val=0)\n",
    "\n",
    "    def forward(self, x,y):\n",
    "        #print(\"0x\",x.size())\n",
    "        #print(\"0y\",y.size())\n",
    "        #x = torch.reshape(x,(self.batch_size,self.latent_dim,1,1))\n",
    "        #y = torch.reshape(y,(self.batch_size,1,1,1))\n",
    "        #print(\"1y\",y.size())\n",
    "        #x = torch.flatten(x, 1)\n",
    "        #print(\"xflat\",x.size()) # xflat torch.Size([64128])\n",
    "        #x = torch.cat([x,y], 1)\n",
    "        #print(\"1x\",x.size()) ## 1x torch.Size([128, 501, 1, 1])\n",
    "        # x = self.LeakyReLU(self.fc_1(x))\n",
    "        #print(\"2x\",x.size()) #2x torch.Size([128, 501])\n",
    "        # x = self.LeakyReLU(self.fc_2(x))\n",
    "        #x = torch.reshape(x,(self.hidden_dim,self.latent_dim,1,1))\n",
    "        #print(\"3x\",x.size())\n",
    "        #x = x.view([-1,self.latent_dim+1,1,1])\n",
    "        #print(\"xres\",x.size())\n",
    "        print(x.shape)\n",
    "        print(y.shape)\n",
    "        \n",
    "        x = self.cntv_1(x)\n",
    "        # print(\"2\",x.size())\n",
    "        x = self.cntv_2(x)\n",
    "        # print(\"3\",x.size())#\n",
    "        x = self.cntv_3(x)\n",
    "        # print(\"4\",x.size())#\n",
    "        x = self.cntv_4(x)\n",
    "        #print(\"5\",x.size())#\n",
    "        x = self.cntv_5(x)\n",
    "        #print(\"6\",x.size())#\n",
    "        x = self.cntv_6(x)\n",
    "        #print(\"6\",x.size())#\n",
    "        x = x.view([-1, 3, 32, 32])\n",
    "        return torch.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Cond_Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_of_classes,batch_size,SPARSITY_CNN,BOOST_STRENGTH,PERCENT_ON,relu_flag):\n",
    "        super(Cond_Discriminator, self).__init__()\n",
    "        self.input_dim = int(input_dim/32/32)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_of_classes = num_of_classes\n",
    "        self.relu_flag= relu_flag\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.cnn_1 = nn.Sequential(\n",
    "            nn.Conv2d(self.input_dim, self.hidden_dim, kernel_size=(3,3), stride=(2,2), padding=(1,1), bias=False),\n",
    "            nn.LeakyReLU(drop_out, inplace=True)\n",
    "        )\n",
    "\n",
    "        self.cnn_2 = nn.Sequential(\n",
    "            nn.Conv2d(self.hidden_dim, self.hidden_dim*2, kernel_size=(4,4), stride=(2,2), padding=(1,1), bias=False),\n",
    "        nn.BatchNorm2d(self.hidden_dim * 2),\n",
    "        KWinners2d(channels=int(self.hidden_dim * 2), percent_on=PERCENT_ON, boost_strength=BOOST_STRENGTH,relu=self.relu_flag),\n",
    "        #nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.cnn_3 = nn.Sequential(\n",
    "            nn.Conv2d(self.hidden_dim*2, self.hidden_dim*4, kernel_size=(4,4), stride=(2,2), padding=(1,1), bias=False),\n",
    "        nn.BatchNorm2d(self.hidden_dim * 4),\n",
    "        KWinners2d(channels=int(self.hidden_dim * 4), percent_on=PERCENT_ON, boost_strength=BOOST_STRENGTH,relu=self.relu_flag),\n",
    "        #nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.cnn_4 = nn.Sequential(\n",
    "            nn.Conv2d(self.hidden_dim * 4, 1, kernel_size=(4,4), stride=(1,1), padding=(0,0), bias=False)\n",
    "        )\n",
    "\n",
    "        # self.sparse_cnn_4 = nn.Sequential(\n",
    "        #     nn.Conv2d(self.hidden_dim * 4, self.input_dim + self.num_of_classes, kernel_size=(4,4), stride=(1,1), padding=(0,0), bias=False)\n",
    "        # )\n",
    "\n",
    "        # self.fc_1 = nn.Linear(2560, int(self.hidden_dim))\n",
    "        # self.fc_2 = nn.Linear(int(self.hidden_dim), int(self.hidden_dim))\n",
    "        # self.fc_out  = nn.Linear(int(self.hidden_dim),self.batch_size)\n",
    "        #\n",
    "        # self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "    #     self.weights_init()\n",
    "    def weights_init(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "            torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "        if isinstance(m, nn.BatchNorm2d):\n",
    "            torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "            torch.nn.init.constant_(m.bias, val=0)\n",
    "    def forward(self, x, y):\n",
    "        #print(\"y\",y.size())\n",
    "        #print(\"x\",x.size())\n",
    "\n",
    "        #print(\"0\",x.size())\n",
    "        x = self.cnn_1(x)\n",
    "        #print(\"1\",x.size())\n",
    "        x = self.cnn_2(x)\n",
    "        x = self.cnn_3(x)\n",
    "        #print(\"2\",x.size())\n",
    "        x = self.cnn_4(x)\n",
    "        #print(\"3\",x.size())# 3 torch.Size([512, 4, 1, 1])\n",
    "        #print(\"y\",y.size())# y torch.Size([512, 1])\n",
    "        #y = torch.reshape(y,(self.batch_size,1,1,1))\n",
    "        #print(\"yr\",y.size())\n",
    "        # x = torch.reshape(x,(512,4))\n",
    "        #\n",
    "        # #print(\"xf\",x.size())\n",
    "        # x = torch.cat([x,y], 1)\n",
    "        # x = torch.flatten(x)\n",
    "        #\n",
    "        #\n",
    "        # #print(\"4\",x.size()) # 4 torch.Size([2560])\n",
    "        # x = self.LeakyReLU(self.fc_1(x))\n",
    "        # #print(\"5\",x.size())\n",
    "        # x = self.LeakyReLU(self.fc_2(x))\n",
    "        # #print(\"6\",x.size())\n",
    "        # x = self.fc_out(x)\n",
    "        #print(\"4\",x.size())\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cond_generator = Cond_Generator(batch_size=batch_size,latent_dim=latent_dim, hidden_dim=128, output_dim=img_size, num_of_classes=len(dataset.classes),SPARSITY_CNN=SPARSITY_CNN,BOOST_STRENGTH=BOOST_STRENGTH,PERCENT_ON=PERCENT_ON,relu_flag=relu_flag).to(device)\n",
    "cond_discriminator = Cond_Discriminator( hidden_dim=32, input_dim=img_size, num_of_classes=len(dataset.classes),batch_size=batch_size,SPARSITY_CNN=SPARSITY_CNN,BOOST_STRENGTH=BOOST_STRENGTH,PERCENT_ON=PERCENT_ON,relu_flag=relu_flag).to(device)\n",
    "\n",
    "# Optimizers\n",
    "generator_optimizer = torch.optim.Adam(cond_generator.parameters(), lr=0.0001)\n",
    "# generator_optimizer = torch.optim.RMSprop(cond_generator.parameters(), lr=0.00005)\n",
    "generator_scheduler = optim.lr_scheduler.ExponentialLR(optimizer=generator_optimizer, gamma=0.99)\n",
    "discriminator_optimizer = torch.optim.Adam(cond_discriminator.parameters(), lr=0.0001)\n",
    "# discriminator_optimizer = torch.optim.RMSprop(cond_discriminator.parameters(), lr=0.00005)\n",
    "discriminator_scheduler = optim.lr_scheduler.ExponentialLR(optimizer=discriminator_optimizer, gamma=0.99)\n",
    "\n",
    "# loss\n",
    "#criterion = nn.MSELoss()\n",
    "criterion  = nn.BCELoss()\n",
    "#criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = torch.randn(32, latent_dim,1,1,device=device)\n",
    "fixed_labels = torch.randint(len(dataset.classes),(32,),device=device)\n",
    "fixed_labels = F.one_hot(fixed_labels, len(dataset.classes)).float()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Parametres - G :  8631168\n",
      "No. of Trainable parametres - G :  8631168\n",
      "No. of Parametres - D :  167136\n",
      "No. of Trainable parametres - D :  167136\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File \"/Users/mac/Documents/studia/SSNE/venv/SSNE/lib/python3.9/site-packages/nupic/torch/functions/k_winners.py\", line 219, in kwinners2d\n                                         keepdim=True)[0]\n        else:\n            threshold = boosted.view(x.shape[0], -1).kthvalue(\n                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n                x.shape[1] * x.shape[2] * x.shape[3] - k + 1, dim=1)[0]\n            threshold = threshold.view(x.shape[0], 1, 1, 1)\nRuntimeError: The operator 'aten::kthvalue.values' is not current implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m label \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfull((b_size,),\u001b[39m0.9\u001b[39m, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat, device\u001b[39m=\u001b[39mdevice) \u001b[39m# Setting labels for real images & preventing overconfidence\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39m# Forward pass real batch through D\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m output \u001b[39m=\u001b[39m cond_discriminator(real_images, y)\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[39m# Calculate loss on all-real batch\u001b[39;00m\n\u001b[1;32m     39\u001b[0m error_discriminator_real \u001b[39m=\u001b[39m criterion(output, label)\n",
      "File \u001b[0;32m~/Documents/studia/SSNE/venv/SSNE/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [13], line 56\u001b[0m, in \u001b[0;36mCond_Discriminator.forward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     54\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcnn_1(x)\n\u001b[1;32m     55\u001b[0m \u001b[39m#print(\"1\",x.size())\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcnn_2(x)\n\u001b[1;32m     57\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcnn_3(x)\n\u001b[1;32m     58\u001b[0m \u001b[39m#print(\"2\",x.size())\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/studia/SSNE/venv/SSNE/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/studia/SSNE/venv/SSNE/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/Documents/studia/SSNE/venv/SSNE/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/studia/SSNE/venv/SSNE/lib/python3.9/site-packages/nupic/torch/modules/k_winners.py:351\u001b[0m, in \u001b[0;36mKWinners2d.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_inference \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mround\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpercent_on_inference))\n\u001b[1;32m    350\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[0;32m--> 351\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mkwinners2d(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mduty_cycle, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mk,\n\u001b[1;32m    352\u001b[0m                      \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cached_boost_strength, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlocal,\n\u001b[1;32m    353\u001b[0m                      \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbreak_ties, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrelu, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n\u001b[1;32m    354\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_duty_cycle(x)\n\u001b[1;32m    355\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File \"/Users/mac/Documents/studia/SSNE/venv/SSNE/lib/python3.9/site-packages/nupic/torch/functions/k_winners.py\", line 219, in kwinners2d\n                                         keepdim=True)[0]\n        else:\n            threshold = boosted.view(x.shape[0], -1).kthvalue(\n                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n                x.shape[1] * x.shape[2] * x.shape[3] - k + 1, dim=1)[0]\n            threshold = threshold.view(x.shape[0], 1, 1, 1)\nRuntimeError: The operator 'aten::kthvalue.values' is not current implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.\n"
     ]
    }
   ],
   "source": [
    "G_losses = []\n",
    "D_losses = []\n",
    "\n",
    "################# NO. PARAMS ####################\n",
    "G_total_params = sum(p.numel() for p in cond_generator.parameters() if p.requires_grad)\n",
    "print(\"No. of Parametres - G : \", G_total_params)\n",
    "G_total_params = sum(p.numel() for p in cond_generator.parameters() if p.requires_grad)\n",
    "print(\"No. of Trainable parametres - G : \", G_total_params)\n",
    "\n",
    "D_total_params = sum(p.numel() for p in cond_discriminator.parameters() if p.requires_grad)\n",
    "print(\"No. of Parametres - D : \", D_total_params)\n",
    "D_total_params = sum(p.numel() for p in cond_discriminator.parameters() if p.requires_grad)\n",
    "print(\"No. of Trainable parametres - D : \", D_total_params)\n",
    "################# NO. PARAMS ####################\n",
    "\n",
    "num_epochs = 40\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in the dataloader\n",
    "    discriminator_fake_acc = []\n",
    "    discriminator_real_acc = []\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        discriminator_optimizer.zero_grad()\n",
    "        # Format batch\n",
    "        real_images = data[0].to(device)\n",
    "        b_size = real_images.size(0)\n",
    "        #for _ in range(disc_iter):\n",
    "        y = data[1]\n",
    "        y = F.one_hot(y, num_classes=len(dataset.classes)).to(device).float()\n",
    "\n",
    "        label = torch.full((b_size,),0.9, dtype=torch.float, device=device) # Setting labels for real images & preventing overconfidence\n",
    "        # Forward pass real batch through D\n",
    "        output = cond_discriminator(real_images, y).view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        error_discriminator_real = criterion(output, label)\n",
    "        #error_discriminator_real = torch.mean(output)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        error_discriminator_real.backward()\n",
    "        discriminator_real_acc.append(output.mean().item())\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(b_size, latent_dim,1,1,device=device)\n",
    "        rand_y = torch.randint(len(dataset.classes),(b_size,),device=device)\n",
    "        rand_y = F.one_hot(rand_y, len(dataset.classes)).float()\n",
    "        # Generate fake image batch with Generator\n",
    "        fake_images = cond_generator(noise, rand_y)\n",
    "        label_fake = torch.full((b_size,),0.1, dtype=torch.float, device=device)\n",
    "        # Classify all fake batch with Discriminator\n",
    "        output = cond_discriminator(fake_images.detach(), rand_y).view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        error_discriminator_fake = criterion(output, label_fake)\n",
    "        #error_discriminator_fake = torch.mean(output)\n",
    "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "        error_discriminator_fake.backward()\n",
    "        discriminator_fake_acc.append(output.mean().item())\n",
    "        # Compute error of D as sum over the fake and the real batches\n",
    "        error_discriminator = error_discriminator_real + error_discriminator_fake\n",
    "        # Update D\n",
    "        #error_discriminator.backward()\n",
    "        discriminator_optimizer.step()\n",
    "\n",
    "        # for p in cond_discriminator.parameters():\n",
    "        #         p.data.clamp_(-WEIGHT_CLIP, WEIGHT_CLIP)\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "#         for _ in range(3):\n",
    "        noise = torch.randn(b_size, latent_dim,1,1,device=device)\n",
    "        rand_y = torch.randint(len(dataset.classes),(b_size,),device=device)\n",
    "        rand_y = F.one_hot(rand_y, len(dataset.classes)).float()\n",
    "        fake_images = cond_generator(noise, rand_y)\n",
    "        generator_optimizer.zero_grad()\n",
    "        label = torch.full((b_size,),1., dtype=torch.float, device=device)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output = cond_discriminator(fake_images, rand_y).view(-1)\n",
    "        # Calculate G's loss based on this output\n",
    "        error_generator = criterion(output, label)\n",
    "        #error_generator = torch.mean(output)\n",
    "        # Calculate gradients for G\n",
    "        error_generator.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        generator_optimizer.step()\n",
    "\n",
    "        # Output training stats\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(error_generator.item())\n",
    "        D_losses.append(error_discriminator.item())\n",
    "\n",
    "\n",
    "    print(f\"Epoch: {epoch}, discrimiantor fake error: {np.mean(discriminator_fake_acc):.3}, discriminator real acc: {np.mean(discriminator_real_acc):.3}\")\n",
    "    generator_scheduler.step()\n",
    "    discriminator_scheduler.step()\n",
    "    cond_generator.apply(update_boost_strength)\n",
    "    cond_discriminator.apply(update_boost_strength)\n",
    "    if epoch % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            fake = cond_generator(fixed_noise, fixed_labels).detach().cpu()\n",
    "        grid = torchvision.utils.make_grid(fake,nrow=8, normalize=True)\n",
    "        grid = grid.permute(1, 2, 0)\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.title(f\"Generations\")\n",
    "        plt.imshow(grid)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generowanie wynikÃ³w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(number_of_images, clazz=None):\n",
    "    fixed_noise = torch.randn(number_of_images, latent_dim, device=device)\n",
    "    fixed_labels = torch.randint(len(dataset.classes), (number_of_images,), device=device)\n",
    "    if clazz:\n",
    "        fixed_labels = torch.tensor([clazz for _ in range(number_of_images)], device=device)\n",
    "    fixed_labels = F.one_hot(fixed_labels, len(dataset.classes)).float()\n",
    "    with torch.no_grad():\n",
    "        fake = cond_generator(fixed_noise, fixed_labels).detach().cpu()\n",
    "        grid = torchvision.utils.make_grid(fake)\n",
    "        grid = grid.permute(1, 2, 0)\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.title(f\"Generations\")\n",
    "        plt.imshow(grid)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_images(number_of_images=8, clazz=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('SSNE')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ab3be89bae1f862a0275f0ab3a3598208c0843bd6ea236e9b0d6e9edc4260b35"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
