{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Processing c:\\mgr materialy + praca mgr\\4 + 2 semestr\\ssne\\mp5\\nupic.torch-master.zip\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: torch<=1.13,>=1.6 in c:\\program files\\python310\\lib\\site-packages (from nupic.torch==0.0.1.dev0) (1.13.0+cu117)\n",
      "Requirement already satisfied: typing-extensions in c:\\program files\\python310\\lib\\site-packages (from torch<=1.13,>=1.6->nupic.torch==0.0.1.dev0) (4.4.0)\n",
      "Building wheels for collected packages: nupic.torch\n",
      "  Building wheel for nupic.torch (pyproject.toml): started\n",
      "  Building wheel for nupic.torch (pyproject.toml): finished with status 'done'\n",
      "Failed to build nupic.torch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Building wheel for nupic.torch failed: [Errno 13] Permission denied: 'c:\\\\users\\\\piomi\\\\appdata\\\\local\\\\pip\\\\cache\\\\wheels\\\\a9\\\\43\\\\50\\\\48bef4d23516ce1da3a14012eb9f221687e8dedc1703aa0acf\\\\nupic.torch-0.0.1.dev0-py3-none-any.whl'\n",
      "ERROR: Could not build wheels for nupic.torch, which is required to install pyproject.toml-based projects\n",
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "C:\\Users\\piomi\\AppData\\Local\\Temp\\ipykernel_8708\\2023843792.py:12: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('svg', 'pdf') # For export\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Processing c:\\mgr materialy + praca mgr\\4 + 2 semestr\\ssne\\mp5\\nupic.torch-master.zip\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: torch<=1.13,>=1.6 in c:\\program files\\python310\\lib\\site-packages (from nupic.torch==0.0.1.dev0) (1.13.0+cu117)\n",
      "Requirement already satisfied: typing-extensions in c:\\program files\\python310\\lib\\site-packages (from torch<=1.13,>=1.6->nupic.torch==0.0.1.dev0) (4.4.0)\n",
      "Building wheels for collected packages: nupic.torch\n",
      "  Building wheel for nupic.torch (pyproject.toml): started\n",
      "  Building wheel for nupic.torch (pyproject.toml): finished with status 'done'\n",
      "Failed to build nupic.torch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Building wheel for nupic.torch failed: [Errno 13] Permission denied: 'c:\\\\users\\\\piomi\\\\appdata\\\\local\\\\pip\\\\cache\\\\wheels\\\\a9\\\\43\\\\50\\\\48bef4d23516ce1da3a14012eb9f221687e8dedc1703aa0acf\\\\nupic.torch-0.0.1.dev0-py3-none-any.whl'\n",
      "ERROR: Could not build wheels for nupic.torch, which is required to install pyproject.toml-based projects\n",
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "from skimage.util import random_noise\n",
    "\n",
    "## PyTorch\n",
    "## Backprop Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "torch.cuda.empty_cache()\n",
    "# Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "\n",
    "## Hebbian Learning + Backprop\n",
    "!pip install nupic.torch-master.zip\n",
    "from nupic.torch.modules import (\n",
    "    KWinners2d, KWinners, SparseWeights, SparseWeights2d, Flatten,\n",
    "    rezero_weights, update_boost_strength\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations applied on each image => only make them a tensor\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                ])\n",
    "\n",
    "# Loading the training dataset. We need to split it into a training and validation part\n",
    "dataset = ImageFolder(\"trafic_32\", transform=transform)\n",
    "#dataset_t = ImageFolder(\"trafic_32\", transform=transform_t)\n",
    "#dataset_enhanced = torch.utils.data.ConcatDataset([dataset, dataset_t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_loader = data.DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "# Sparsity parameters\n",
    "SPARSITY = 0.66\n",
    "SPARSITY_CNN = SPARSITY\n",
    "\n",
    "# K-Winners parameters\n",
    "PERCENT_ON = 0.5\n",
    "BOOST_STRENGTH = 1.8\n",
    "relu_flag = False\n",
    "\n",
    "# Models\n",
    "latent_dim = 200\n",
    "img_size = 32 * 32 * 3\n",
    "\n",
    "# W-GAN PARAMETERS\n",
    "WEIGHT_CLIP = 0.01\n",
    "disc_iter = 5\n",
    "\n",
    "# Layers\n",
    "drop_out = 0.2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "class Cond_Generator(nn.Module):\n",
    "    def __init__(self, batch_size,latent_dim, hidden_dim, output_dim, num_of_classes,SPARSITY_CNN,BOOST_STRENGTH,PERCENT_ON,relu_flag):\n",
    "        super(Cond_Generator, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = int(output_dim)\n",
    "        self.num_of_classes = num_of_classes\n",
    "        self.relu_flag= relu_flag\n",
    "        self.SPARSITY_CNN = SPARSITY_CNN\n",
    "        #\n",
    "        # self.fc_1 = nn.Sequential(nn.Linear(self.latent_dim+self.num_of_classes, self.latent_dim+self.num_of_classes))\n",
    "        #\n",
    "        # # self.fc_2 = nn.Sequential(SparseWeights(nn.Linear(self.hidden_dim, self.hidden_dim), sparsity=SPARSITY),\n",
    "        # # KWinners(n=self.hidden_dim, percent_on=PERCENT_ON, boost_strength=BOOST_STRENGTH,relu=self.relu_flag),)\n",
    "        #\n",
    "        # self.fc_2 = nn.Sequential(nn.Linear(self.latent_dim+self.num_of_classes, int(self.hidden_dim)))\n",
    "        # self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.cntv_1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(int(self.latent_dim), int(self.hidden_dim*4), kernel_size=(3, 3), stride=(1, 1), padding=(0, 0), bias=False),\n",
    "            KWinners2d(channels=int(self.hidden_dim*4), percent_on=PERCENT_ON, boost_strength=BOOST_STRENGTH,relu=self.relu_flag),\n",
    "            nn.Dropout(drop_out)\n",
    "        )\n",
    "\n",
    "        self.cntv_labels = nn.Sequential(\n",
    "            nn.ConvTranspose2d(int(self.num_of_classes), int(self.hidden_dim*4), kernel_size=(3, 3), stride=(1, 1), padding=(0, 0), bias=False),\n",
    "            KWinners2d(channels=int(self.hidden_dim*4), percent_on=PERCENT_ON, boost_strength=BOOST_STRENGTH,relu=self.relu_flag),\n",
    "            nn.Dropout(drop_out)\n",
    "        )\n",
    "\n",
    "        self.cntv_2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d( int(self.hidden_dim*8), int(self.hidden_dim*4), kernel_size=(3,3), stride=(2,2), padding=(1,1), bias=False),\n",
    "            nn.BatchNorm2d(int(self.hidden_dim*4)),\n",
    "            #nn.InstanceNorm2d(int(self.hidden_dim*4)),\n",
    "            KWinners2d(channels=int(self.hidden_dim*4), percent_on=PERCENT_ON, boost_strength=BOOST_STRENGTH,relu=self.relu_flag),\n",
    "            nn.Dropout(drop_out)\n",
    "        )\n",
    "\n",
    "        self.cntv_3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(int(self.hidden_dim*4), int(self.hidden_dim*2), kernel_size=(3,3), stride=(2,2), padding=(1,1), bias=False),\n",
    "            nn.BatchNorm2d(int(self.hidden_dim*2)),\n",
    "            #nn.InstanceNorm2d(int(self.hidden_dim*2)),\n",
    "            KWinners2d(channels=int(self.hidden_dim*2), percent_on=PERCENT_ON, boost_strength=BOOST_STRENGTH,relu=self.relu_flag),\n",
    "            nn.Dropout(drop_out),\n",
    "        )\n",
    "\n",
    "        self.cntv_4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(int(self.hidden_dim*2), int(self.hidden_dim), kernel_size=(3,3), stride=(2,2), padding=(1,1), bias=False),\n",
    "            nn.BatchNorm2d(int(self.hidden_dim)),\n",
    "            #nn.InstanceNorm2d(int(self.hidden_dim)),\n",
    "            KWinners2d(channels=int(self.hidden_dim), percent_on=PERCENT_ON, boost_strength=BOOST_STRENGTH,relu=self.relu_flag),\n",
    "            nn.Dropout(drop_out)\n",
    "        )\n",
    "\n",
    "        self.cntv_5 =  nn.Sequential(\n",
    "            nn.ConvTranspose2d(int(self.hidden_dim), int(self.hidden_dim/2), kernel_size=(3,3), stride=(2,2), padding=(1,1), bias=False),\n",
    "            nn.BatchNorm2d(int(self.hidden_dim/2)),\n",
    "            #nn.InstanceNorm2d(int(self.hidden_dim/2)),\n",
    "            KWinners2d(channels=int(self.hidden_dim/2), percent_on=PERCENT_ON, boost_strength=BOOST_STRENGTH,relu=self.relu_flag),\n",
    "            nn.Dropout(drop_out)\n",
    "        )\n",
    "\n",
    "        self.cntv_6 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(int(self.hidden_dim/2), int(self.output_dim/32/32), kernel_size=(4,4), stride=(1,1), padding=(2,2), bias=False),\n",
    "            nn.BatchNorm2d(int(self.output_dim/32/32)),#https://arxiv.org/pdf/1805.07389.pdf -> in fact better results\n",
    "            #nn.InstanceNorm2d(int(self.output_dim/32/32))\n",
    "            # https://arxiv.org/pdf/1812.04948.pdf\n",
    "\n",
    "        )\n",
    "        self.weights_init()\n",
    "    def weights_init(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "            torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "        if isinstance(m, nn.BatchNorm2d):\n",
    "            torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "            torch.nn.init.constant_(m.bias, val=0)\n",
    "\n",
    "    def forward(self, x,y):\n",
    "        #print(\"0x\",x.size())\n",
    "        #print(\"0y\",y.size())\n",
    "        #x = torch.reshape(x,(self.batch_size,self.latent_dim,1,1))\n",
    "        y = y.view([-1,self.num_of_classes,1,1])\n",
    "        #print(\"1y\",y.size())\n",
    "        #x = torch.flatten(x, 1)\n",
    "        #print(\"xflat\",x.size()) # xflat torch.Size([64128])\n",
    "        #x = torch.cat([x,y], 1)\n",
    "        #print(\"1x\",x.size()) ## 1x torch.Size([128, 501, 1, 1])\n",
    "        # x = self.LeakyReLU(self.fc_1(x))\n",
    "        #print(\"2x\",x.size()) #2x torch.Size([128, 501])\n",
    "        # x = self.LeakyReLU(self.fc_2(x))\n",
    "        #x = torch.reshape(x,(self.hidden_dim,self.latent_dim,1,1))\n",
    "        #print(\"3x\",x.size())\n",
    "        #x = x.view([-1,self.latent_dim+1,1,1])\n",
    "        #print(\"xres\",x.size())\n",
    "        x = self.cntv_1(x)\n",
    "        y = self.cntv_labels(y)\n",
    "        #print(\"2x\",x.size())\n",
    "        #print(\"2y\",y.size())\n",
    "        x = torch.cat([x,y], 1)\n",
    "        #print(\"3x\",x.size())#\n",
    "        x = self.cntv_2(x)\n",
    "        # print(\"3\",x.size())#\n",
    "        x = self.cntv_3(x)\n",
    "        # print(\"4\",x.size())#\n",
    "        x = self.cntv_4(x)\n",
    "        #print(\"5\",x.size())#\n",
    "        x = self.cntv_5(x)\n",
    "        #print(\"6\",x.size())#\n",
    "        x = self.cntv_6(x)\n",
    "        #print(\"6\",x.size())#\n",
    "        x = x.view([-1, 3, 32, 32])\n",
    "        return torch.tanh(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "class Cond_Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_of_classes,batch_size,SPARSITY_CNN,BOOST_STRENGTH,PERCENT_ON,relu_flag):\n",
    "        super(Cond_Discriminator, self).__init__()\n",
    "        self.input_dim = int(input_dim/32/32)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_of_classes = num_of_classes\n",
    "        self.relu_flag= relu_flag\n",
    "        self.batch_size = batch_size\n",
    "        self.SPARSITY_CNN = SPARSITY_CNN\n",
    "\n",
    "        self.cnn_1 = nn.Sequential(\n",
    "            nn.Conv2d(self.input_dim, self.hidden_dim, kernel_size=(3,3), stride=(2,2), padding=(1,1), bias=False),\n",
    "            nn.LeakyReLU(drop_out, inplace=True)\n",
    "        )\n",
    "\n",
    "        self.cnn_labels = nn.Sequential(\n",
    "            nn.Conv2d(self.num_of_classes, self.hidden_dim, kernel_size=(3,3), stride=(2,2), padding=(1,1), bias=False),\n",
    "            nn.LeakyReLU(drop_out, inplace=True)\n",
    "        )\n",
    "\n",
    "        self.cnn_2 = nn.Sequential(\n",
    "            nn.Conv2d(self.hidden_dim*2, self.hidden_dim*4, kernel_size=(4,4), stride=(2,2), padding=(1,1), bias=False),\n",
    "        nn.BatchNorm2d(self.hidden_dim * 4),\n",
    "        KWinners2d(channels=int(self.hidden_dim * 4), percent_on=PERCENT_ON, boost_strength=BOOST_STRENGTH,relu=self.relu_flag),\n",
    "        #nn.LeakyReLU(0.2, inplace=True),\n",
    "        nn.Dropout(drop_out),\n",
    "        )\n",
    "\n",
    "        self.cnn_3 = nn.Sequential(\n",
    "            nn.Conv2d(self.hidden_dim*4, self.hidden_dim*8, kernel_size=(4,4), stride=(2,2), padding=(1,1), bias=False),\n",
    "        nn.BatchNorm2d(self.hidden_dim * 8),\n",
    "        KWinners2d(channels=int(self.hidden_dim * 8), percent_on=PERCENT_ON, boost_strength=BOOST_STRENGTH,relu=self.relu_flag),\n",
    "        #nn.LeakyReLU(0.2, inplace=True),\n",
    "        nn.Dropout(drop_out),\n",
    "        )\n",
    "\n",
    "        self.cnn_4 = nn.Sequential(\n",
    "            nn.Conv2d(self.hidden_dim * 8, 1, kernel_size=(4,4), stride=(1,1), padding=(0,0), bias=False)\n",
    "        )\n",
    "\n",
    "        # self.sparse_cnn_4 = nn.Sequential(\n",
    "        #     nn.Conv2d(self.hidden_dim * 4, self.input_dim + self.num_of_classes, kernel_size=(4,4), stride=(1,1), padding=(0,0), bias=False)\n",
    "        # )\n",
    "\n",
    "        # self.fc_1 = nn.Linear(2560, int(self.hidden_dim))\n",
    "        # self.fc_2 = nn.Linear(int(self.hidden_dim), int(self.hidden_dim))\n",
    "        # self.fc_out  = nn.Linear(int(self.hidden_dim),self.batch_size)\n",
    "        #\n",
    "        # self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "    #     self.weights_init()\n",
    "    def weights_init(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "            torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "        if isinstance(m, nn.BatchNorm2d):\n",
    "            torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "            torch.nn.init.constant_(m.bias, val=0)\n",
    "    def forward(self, x, y):\n",
    "        #print(\"y\",y.size())\n",
    "        #print(\"x\",x.size())\n",
    "        y = y.unsqueeze(2).repeat([1, 1, 32])\n",
    "        y = y.unsqueeze(3).repeat([1, 1, 1, 32])\n",
    "        #print(\"0\",x.size())\n",
    "        x = self.cnn_1(x)\n",
    "        y = self.cnn_labels(y)\n",
    "        x = torch.cat([x,y], 1)\n",
    "        #print(\"1\",x.size())\n",
    "        x = self.cnn_2(x)\n",
    "        x = self.cnn_3(x)\n",
    "        #print(\"2\",x.size())\n",
    "        x = self.cnn_4(x)\n",
    "        #print(\"3\",x.size())# 3 torch.Size([512, 4, 1, 1])\n",
    "        #print(\"y\",y.size())# y torch.Size([512, 1])\n",
    "        #y = torch.reshape(y,(self.batch_size,1,1,1))\n",
    "        #print(\"yr\",y.size())\n",
    "        # x = torch.reshape(x,(512,4))\n",
    "        #\n",
    "        # #print(\"xf\",x.size())\n",
    "        # x = torch.cat([x,y], 1)\n",
    "        # x = torch.flatten(x)\n",
    "        #\n",
    "        #\n",
    "        # #print(\"4\",x.size()) # 4 torch.Size([2560])\n",
    "        # x = self.LeakyReLU(self.fc_1(x))\n",
    "        # #print(\"5\",x.size())\n",
    "        # x = self.LeakyReLU(self.fc_2(x))\n",
    "        # #print(\"6\",x.size())\n",
    "        # x = self.fc_out(x)\n",
    "        #print(\"4\",x.size())\n",
    "        return torch.sigmoid(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cond_generator = Cond_Generator(batch_size=batch_size,latent_dim=latent_dim, hidden_dim=128, output_dim=img_size, num_of_classes=len(dataset.classes),SPARSITY_CNN=SPARSITY_CNN,BOOST_STRENGTH=BOOST_STRENGTH,PERCENT_ON=PERCENT_ON,relu_flag=relu_flag).to(device)\n",
    "cond_discriminator = Cond_Discriminator( hidden_dim=64, input_dim=img_size, num_of_classes=len(dataset.classes),batch_size=batch_size,SPARSITY_CNN=SPARSITY_CNN,BOOST_STRENGTH=BOOST_STRENGTH,PERCENT_ON=PERCENT_ON,relu_flag=relu_flag).to(device)\n",
    "\n",
    "# Optimizers\n",
    "generator_optimizer = torch.optim.Adam(cond_generator.parameters(), lr=0.0001)\n",
    "# generator_optimizer = torch.optim.RMSprop(cond_generator.parameters(), lr=0.00005)\n",
    "generator_scheduler = optim.lr_scheduler.ExponentialLR(optimizer=generator_optimizer, gamma=0.99)\n",
    "discriminator_optimizer = torch.optim.Adam(cond_discriminator.parameters(), lr=0.0001)\n",
    "# discriminator_optimizer = torch.optim.RMSprop(cond_discriminator.parameters(), lr=0.00005)\n",
    "discriminator_scheduler = optim.lr_scheduler.ExponentialLR(optimizer=discriminator_optimizer, gamma=0.99)\n",
    "\n",
    "# loss\n",
    "#criterion = nn.MSELoss()\n",
    "#criterion  = nn.BCELoss(reduction=\"mean\")\n",
    "#criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = torch.randn(32, latent_dim,1,1,device=device)\n",
    "fixed_labels = torch.randint(len(dataset.classes),(32,),device=device)\n",
    "fixed_labels = F.one_hot(fixed_labels, len(dataset.classes)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "from torch import autograd\n",
    "# https://arxiv.org/pdf/1704.00028.pdf\n",
    "def compute_gp(netD, real_data, fake_data,rand_y):\n",
    "        batch_size = real_data.size(0)\n",
    "        eps = torch.rand(batch_size, 1, 1, 1).to(real_data.device)\n",
    "        eps = eps.expand_as(real_data)\n",
    "        interpolation = eps * real_data + (1 - eps) * fake_data\n",
    "        interp_logits = netD(interpolation,rand_y)\n",
    "        grad_outputs = torch.ones_like(interp_logits)\n",
    "        gradients = autograd.grad(\n",
    "            outputs=interp_logits,\n",
    "            inputs=interpolation,\n",
    "            grad_outputs=grad_outputs,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "        )[0]\n",
    "\n",
    "        # Compute and return Gradient Norm\n",
    "        gradients = gradients.view(batch_size, -1)\n",
    "        grad_norm = gradients.norm(2, 1)\n",
    "        return torch.mean((grad_norm - 1) ** 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Parametres - G :  7198086\n",
      "No. of Trainable parametres - G :  7198086\n",
      "No. of Parametres - D :  2633472\n",
      "No. of Trainable parametres - D :  2633472\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [83], line 33\u001B[0m\n\u001B[0;32m     31\u001B[0m discriminator_optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     32\u001B[0m \u001B[38;5;66;03m# Format batch\u001B[39;00m\n\u001B[1;32m---> 33\u001B[0m real_images \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     34\u001B[0m b_size \u001B[38;5;241m=\u001B[39m real_images\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(disc_iter):\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "G_losses = []\n",
    "D_losses = []\n",
    "\n",
    "GPATH = \"WGAN-Clipping-K-Win_G0.pt\"\n",
    "DPATH = \"WGAN-Clipping-K-Win_D0.pt\"\n",
    "\n",
    "\n",
    "################# NO. PARAMS ####################\n",
    "G_total_params = sum(p.numel() for p in cond_generator.parameters() if p.requires_grad)\n",
    "print(\"No. of Parametres - G : \", G_total_params)\n",
    "G_total_params = sum(p.numel() for p in cond_generator.parameters() if p.requires_grad)\n",
    "print(\"No. of Trainable parametres - G : \", G_total_params)\n",
    "\n",
    "D_total_params = sum(p.numel() for p in cond_discriminator.parameters() if p.requires_grad)\n",
    "print(\"No. of Parametres - D : \", D_total_params)\n",
    "D_total_params = sum(p.numel() for p in cond_discriminator.parameters() if p.requires_grad)\n",
    "print(\"No. of Trainable parametres - D : \", D_total_params)\n",
    "################# NO. PARAMS ####################\n",
    "\n",
    "num_epochs = 61\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in the dataloader\n",
    "    discriminator_fake_acc = []\n",
    "    discriminator_real_acc = []\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        discriminator_optimizer.zero_grad()\n",
    "        # Format batch\n",
    "        real_images = data[0].to(device)\n",
    "        b_size = real_images.size(0)\n",
    "        for _ in range(disc_iter):\n",
    "            y = data[1]\n",
    "            y = F.one_hot(y, num_classes=len(dataset.classes)).to(device).float()\n",
    "\n",
    "            #####label = torch.full((b_size,),0.9, dtype=torch.float, device=device) # Setting non 1 labels for real images & preventing overconfidence\n",
    "            # Forward pass real batch through D\n",
    "            real_preds = cond_discriminator(real_images, y).view(-1)\n",
    "            # Calculate loss on all-real batch\n",
    "            #error_discriminator_real = -criterion(real_preds, label)\n",
    "            error_discriminator_real = -torch.mean(real_preds)\n",
    "            # Calculate gradients for D in backward pass\n",
    "            #error_discriminator_real.backward()\n",
    "            discriminator_real_acc.append(real_preds.mean().item())\n",
    "\n",
    "            ## Train with all-fake batch\n",
    "            # Generate batch of latent vectors\n",
    "            noise = torch.randn(b_size, latent_dim,1,1,device=device)\n",
    "            rand_y = torch.randint(len(dataset.classes),(b_size,),device=device)\n",
    "            rand_y = F.one_hot(rand_y, len(dataset.classes)).float()\n",
    "            # Generate fake image batch with Generator\n",
    "            fake_images = cond_generator(noise, rand_y)\n",
    "            #####label_fake = torch.full((b_size,),0.05, dtype=torch.float, device=device)# Setting non 0 labels for fake images & preventing overconfidence\n",
    "            # Classify all fake batch with Discriminator\n",
    "            fake_preds = cond_discriminator(fake_images.detach(), rand_y).view(-1)\n",
    "            # Calculate D's loss on the all-fake batch\n",
    "            #error_discriminator_fake = criterion(fake_preds, label_fake)\n",
    "            error_discriminator_fake = torch.mean(fake_preds)\n",
    "            # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "            #error_discriminator_fake.backward()\n",
    "            discriminator_fake_acc.append(fake_preds.mean().item())\n",
    "            # Compute gradient Penalty\n",
    "            #gp = compute_gp(cond_discriminator,real_images,fake_images,rand_y)\n",
    "            # Compute error of D as sum over the fake and the real batches\n",
    "            error_discriminator = error_discriminator_real + error_discriminator_fake #+ gp*0.2\n",
    "            # Update D\n",
    "            error_discriminator.backward()\n",
    "            discriminator_optimizer.step()\n",
    "            D_losses.append(error_discriminator.item())\n",
    "            # Weigth clipping from WGAN Paper (with code)\n",
    "            # for p in cond_discriminator.parameters():\n",
    "            #     p.data.clamp_(-WEIGHT_CLIP , WEIGHT_CLIP )\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        generator_optimizer.zero_grad()\n",
    "        noise = torch.randn(b_size, latent_dim,1,1,device=device)\n",
    "        rand_y = torch.randint(len(dataset.classes),(b_size,),device=device)\n",
    "        rand_y = F.one_hot(rand_y, len(dataset.classes)).float()\n",
    "        fake_images = cond_generator(noise, rand_y)\n",
    "\n",
    "        label = torch.full((b_size,),1., dtype=torch.float, device=device)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        predictions = cond_discriminator(fake_images, rand_y).view(-1)\n",
    "        # Calculate G's loss based on this output\n",
    "        #error_generator = -criterion(predictions, label)\n",
    "        error_generator = -torch.mean(predictions)\n",
    "        # Calculate gradients for G\n",
    "        error_generator.backward()\n",
    "\n",
    "        D_G_z2 = predictions.mean().item()\n",
    "        # Update G\n",
    "        generator_optimizer.step()\n",
    "        # Output training stats\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(error_generator.item())\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Epoch: {epoch}, discrimiantor fake error: {np.mean(discriminator_fake_acc):.3}, discriminator real acc: {np.mean(discriminator_real_acc):.3}\")\n",
    "    generator_scheduler.step()\n",
    "    discriminator_scheduler.step()\n",
    "    cond_generator.apply(update_boost_strength)\n",
    "    cond_discriminator.apply(update_boost_strength)\n",
    "    if epoch % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            fake = cond_generator(fixed_noise, fixed_labels).detach().cpu()\n",
    "        grid = torchvision.utils.make_grid(fake,nrow=8, normalize=True)\n",
    "        grid = grid.permute(1, 2, 0)\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.title(f\"Generations\")\n",
    "        plt.imshow(grid)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        # SAVE MODEL\n",
    "        torch.save(cond_generator, GPATH)\n",
    "        torch.save(cond_discriminator, DPATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generowanie wyników"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [75], line 5\u001B[0m\n\u001B[0;32m      2\u001B[0m DPATH \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWGAN-Clipping-K-Win_D0.pt\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# SAVE MODEL\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m torch\u001B[38;5;241m.\u001B[39msave(cond_generator, GPATH)\n\u001B[0;32m      6\u001B[0m torch\u001B[38;5;241m.\u001B[39msave(cond_discriminator, DPATH)\n",
      "File \u001B[1;32mC:\\Program Files\\Python310\\lib\\site-packages\\torch\\serialization.py:423\u001B[0m, in \u001B[0;36msave\u001B[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001B[0m\n\u001B[0;32m    421\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _use_new_zipfile_serialization:\n\u001B[0;32m    422\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m _open_zipfile_writer(f) \u001B[38;5;28;01mas\u001B[39;00m opened_zipfile:\n\u001B[1;32m--> 423\u001B[0m         \u001B[43m_save\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopened_zipfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpickle_module\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpickle_protocol\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    424\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m    425\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32mC:\\Program Files\\Python310\\lib\\site-packages\\torch\\serialization.py:647\u001B[0m, in \u001B[0;36m_save\u001B[1;34m(obj, zip_file, pickle_module, pickle_protocol)\u001B[0m\n\u001B[0;32m    643\u001B[0m \u001B[38;5;66;03m# given that we copy things around anyway, we might use storage.cpu()\u001B[39;00m\n\u001B[0;32m    644\u001B[0m \u001B[38;5;66;03m# this means to that to get tensors serialized, you need to implement\u001B[39;00m\n\u001B[0;32m    645\u001B[0m \u001B[38;5;66;03m# .cpu() on the underlying Storage\u001B[39;00m\n\u001B[0;32m    646\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m storage\u001B[38;5;241m.\u001B[39mdevice\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m--> 647\u001B[0m     storage \u001B[38;5;241m=\u001B[39m \u001B[43mstorage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcpu\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    648\u001B[0m \u001B[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001B[39;00m\n\u001B[0;32m    649\u001B[0m num_bytes \u001B[38;5;241m=\u001B[39m storage\u001B[38;5;241m.\u001B[39mnbytes()\n",
      "File \u001B[1;32mC:\\Program Files\\Python310\\lib\\site-packages\\torch\\storage.py:120\u001B[0m, in \u001B[0;36m_StorageBase.cpu\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    118\u001B[0m \u001B[38;5;124;03m\"\"\"Returns a CPU copy of this storage if it's not already on the CPU\"\"\"\u001B[39;00m\n\u001B[0;32m    119\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m--> 120\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mUntypedStorage\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcopy_\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    121\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    122\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "# LOAD MODELS\n",
    "# cond_generator = torch.load(GPATH)\n",
    "# cond_discriminator = torch.load(DPATH)\n",
    "# cond_generator.eval()\n",
    "# cond_discriminator.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(number_of_images, clazz=None, show=True):\n",
    "    fixed_noise = torch.randn(number_of_images, latent_dim, 1, 1, device=device)\n",
    "    fixed_labels = torch.randint(len(dataset.classes), (number_of_images,), device=device)\n",
    "    if clazz:\n",
    "        fixed_labels = torch.tensor([clazz for _ in range(number_of_images)], device=device)\n",
    "    fixed_labels = F.one_hot(fixed_labels, len(dataset.classes)).float()\n",
    "    with torch.no_grad():\n",
    "        fake = cond_generator(fixed_noise, fixed_labels).detach().cpu()\n",
    "        # normalizacja z [-1, 1] do [0, 1]\n",
    "        fake = (fake + 1) / 2\n",
    "\n",
    "        if show:\n",
    "            grid = torchvision.utils.make_grid(fake)\n",
    "            grid = grid.permute(1, 2, 0)\n",
    "            plt.figure(figsize=(10,10))\n",
    "            plt.title(f\"Generations\")\n",
    "            plt.imshow(grid)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "        return fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "philox_cuda_state for an unexpected CUDA generator used during capture. In regions captured by CUDA graphs, you may only use the default CUDA RNG generator on the device that's current when capture begins. If you need a non-default (user-supplied) generator, or a generator on another device, please file an issue.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [85], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m images \u001B[38;5;241m=\u001B[39m generate_images(\u001B[38;5;241m1000\u001B[39m, show\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "Cell \u001B[1;32mIn [84], line 2\u001B[0m, in \u001B[0;36mgenerate_images\u001B[1;34m(number_of_images, clazz, show)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_images\u001B[39m(number_of_images, clazz\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, show\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[1;32m----> 2\u001B[0m     fixed_noise \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnumber_of_images\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlatent_dim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m     fixed_labels \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrandint(\u001B[38;5;28mlen\u001B[39m(dataset\u001B[38;5;241m.\u001B[39mclasses), (number_of_images,), device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m clazz:\n",
      "\u001B[1;31mRuntimeError\u001B[0m: philox_cuda_state for an unexpected CUDA generator used during capture. In regions captured by CUDA graphs, you may only use the default CUDA RNG generator on the device that's current when capture begins. If you need a non-default (user-supplied) generator, or a generator on another device, please file an issue."
     ]
    }
   ],
   "source": [
    "images = generate_images(1000, show=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SSNE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08f109b28ac4d856f241b97928d0d5bfddfa33a99d55412376cd8c61c424e303"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}