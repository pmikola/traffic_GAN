{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\piomi\\AppData\\Local\\Temp\\ipykernel_9800\\452679385.py:12: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('svg', 'pdf') # For export\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Processing c:\\mgr materialy + praca mgr\\4 + 2 semestr\\ssne\\mp5\\nupic.torch-master.zip\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: torch<=1.13,>=1.6 in c:\\program files\\python310\\lib\\site-packages (from nupic.torch==0.0.1.dev0) (1.13.0+cu117)\n",
      "Requirement already satisfied: typing-extensions in c:\\program files\\python310\\lib\\site-packages (from torch<=1.13,>=1.6->nupic.torch==0.0.1.dev0) (4.4.0)\n",
      "Building wheels for collected packages: nupic.torch\n",
      "  Building wheel for nupic.torch (pyproject.toml): started\n",
      "  Building wheel for nupic.torch (pyproject.toml): finished with status 'done'\n",
      "Failed to build nupic.torch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Building wheel for nupic.torch failed: [Errno 13] Permission denied: 'c:\\\\users\\\\piomi\\\\appdata\\\\local\\\\pip\\\\cache\\\\wheels\\\\a9\\\\43\\\\50\\\\48bef4d23516ce1da3a14012eb9f221687e8dedc1703aa0acf\\\\nupic.torch-0.0.1.dev0-py3-none-any.whl'\n",
      "ERROR: Could not build wheels for nupic.torch, which is required to install pyproject.toml-based projects\n",
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "\n",
    "## PyTorch\n",
    "## Backprop Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "# Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "\n",
    "## Hebbian Learning\n",
    "!pip install nupic.torch-master.zip\n",
    "from nupic.torch.modules import (\n",
    "    KWinners2d, KWinners, SparseWeights, SparseWeights2d, Flatten,\n",
    "    rezero_weights, update_boost_strength\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations applied on each image => only make them a tensor\n",
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                                AddGaussianNoise(0., 1.)\n",
    "                                ])\n",
    "\n",
    "# Loading the training dataset. We need to split it into a training and validation part\n",
    "dataset = ImageFolder(\"trafic_32\", transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "train_loader = data.DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "outputs": [],
   "source": [
    "# Sparsity parameters\n",
    "SPARSITY = 0.66\n",
    "SPARSITY_CNN = SPARSITY\n",
    "\n",
    "# K-Winners parameters\n",
    "PERCENT_ON = 0.5\n",
    "BOOST_STRENGTH = 1.7\n",
    "relu_flag = True\n",
    "\n",
    "# Models\n",
    "latent_dim = 100\n",
    "img_size = 32 * 32 * 3\n",
    "\n",
    "# W-GAN PARAMETERS\n",
    "WEIGHT_CLIP = 0.01\n",
    "disc_iter = 5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cond_Generator(nn.Module):\n",
    "    def __init__(self, batch_size,latent_dim, hidden_dim, output_dim, num_of_classes,SPARSITY_CNN,BOOST_STRENGTH,PERCENT_ON,relu_flag):\n",
    "        super(Cond_Generator, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = int(output_dim)\n",
    "        self.num_of_classes = num_of_classes\n",
    "        self.relu_flag= relu_flag\n",
    "\n",
    "        #\n",
    "        self.fc_1 = nn.Sequential(nn.Linear(self.latent_dim+self.num_of_classes, self.latent_dim+self.num_of_classes))\n",
    "\n",
    "        # self.fc_2 = nn.Sequential(SparseWeights(nn.Linear(self.hidden_dim, self.hidden_dim), sparsity=SPARSITY),\n",
    "        # KWinners(n=self.hidden_dim, percent_on=PERCENT_ON, boost_strength=BOOST_STRENGTH,relu=self.relu_flag),)\n",
    "\n",
    "        self.fc_2 = nn.Sequential(nn.Linear(self.latent_dim+self.num_of_classes, int(self.hidden_dim)))\n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.cntv_1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(int(self.hidden_dim), int(self.hidden_dim * 4), kernel_size=(3, 3), stride=(1, 1), padding=(0, 0), bias=False),\n",
    "            nn.BatchNorm2d(int(self.hidden_dim*4)),\n",
    "            KWinners2d(channels=int(self.hidden_dim*4), percent_on=PERCENT_ON, boost_strength=BOOST_STRENGTH,relu=self.relu_flag),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        self.cntv_2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d( int(self.hidden_dim*4), int(self.hidden_dim*2), kernel_size=(4,4), stride=(2,2), padding=(1,1), bias=False),\n",
    "            nn.BatchNorm2d(int(self.hidden_dim*2)),\n",
    "            KWinners2d(channels=int(self.hidden_dim*2), percent_on=PERCENT_ON, boost_strength=BOOST_STRENGTH,relu=self.relu_flag),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        self.cntv_3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(int(self.hidden_dim*2), int(self.hidden_dim), kernel_size=(4,4), stride=(2,2), padding=(1,1), bias=False),\n",
    "            nn.BatchNorm2d(int(self.hidden_dim)),\n",
    "            KWinners2d(channels=int(self.hidden_dim), percent_on=PERCENT_ON, boost_strength=BOOST_STRENGTH,relu=self.relu_flag),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        self.cntv_4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(int(self.hidden_dim), int(self.hidden_dim/2), kernel_size=(4,4), stride=(2,2), padding=(1,1), bias=False),\n",
    "            nn.BatchNorm2d(int(self.hidden_dim/2)),\n",
    "            KWinners2d(channels=int(self.hidden_dim/2), percent_on=PERCENT_ON, boost_strength=BOOST_STRENGTH,relu=self.relu_flag),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        self.cntv_5 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(int(self.hidden_dim/2), int(self.output_dim/32/32), kernel_size=(4,4), stride=(2,2), padding=(9,9), bias=False),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        self.weights_init()\n",
    "    def weights_init(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "            torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "        if isinstance(m, nn.BatchNorm2d):\n",
    "            torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "            torch.nn.init.constant_(m.bias, val=0)\n",
    "\n",
    "    def forward(self, x,y):\n",
    "        #print(\"0x\",x.size())\n",
    "\n",
    "        #print(\"0y\",y.size())\n",
    "        #x = torch.reshape(x,(self.batch_size,self.latent_dim,1,1))\n",
    "        #y = torch.reshape(y,(self.batch_size,1,1,1))\n",
    "        #print(\"1y\",y.size())\n",
    "        x = torch.flatten(x, 1)\n",
    "        #print(\"xflat\",x.size()) # xflat torch.Size([64128])\n",
    "        x = torch.cat([x,y], 1)\n",
    "        #print(\"1x\",x.size()) ## 1x torch.Size([128, 501, 1, 1])\n",
    "        x = self.fc_1(x)\n",
    "        #print(\"2x\",x.size()) #2x torch.Size([128, 501])\n",
    "        x = self.fc_2(x)\n",
    "        #x = torch.reshape(x,(self.hidden_dim,self.latent_dim,1,1))\n",
    "        # print(\"3x\",x.size())\n",
    "        x = x.view([-1,self.hidden_dim,1,1])\n",
    "        # print(\"xres\",x.size())\n",
    "        x = self.cntv_1(x)\n",
    "        # print(\"2\",x.size())\n",
    "        x = self.cntv_2(x)\n",
    "        # print(\"3\",x.size())#\n",
    "        x = self.cntv_3(x)\n",
    "        # print(\"4\",x.size())#\n",
    "        x = self.cntv_4(x)\n",
    "        # print(\"5\",x.size())#\n",
    "        x = self.cntv_5(x)\n",
    "        # print(\"6\",x.size())#\n",
    "        x = x.view([-1, 3, 32, 32])\n",
    "        return torch.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "outputs": [],
   "source": [
    "class Cond_Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_of_classes,batch_size,SPARSITY_CNN,BOOST_STRENGTH,PERCENT_ON,relu_flag):\n",
    "        super(Cond_Discriminator, self).__init__()\n",
    "        self.input_dim = int(input_dim/32/32)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_of_classes = num_of_classes\n",
    "        self.relu_flag= relu_flag\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.cnn_1 = nn.Sequential(\n",
    "            nn.Conv2d(self.input_dim, self.hidden_dim, kernel_size=(3,3), stride=(2,2), padding=(1,1), bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.cnn_2 = nn.Sequential(\n",
    "            nn.Conv2d(self.hidden_dim, self.hidden_dim*2, kernel_size=(4,4), stride=(2,2), padding=(1,1), bias=False),\n",
    "        nn.BatchNorm2d(self.hidden_dim * 2),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.cnn_3 = nn.Sequential(\n",
    "            nn.Conv2d(self.hidden_dim*2, self.hidden_dim*4, kernel_size=(4,4), stride=(2,2), padding=(1,1), bias=False),\n",
    "        nn.BatchNorm2d(self.hidden_dim * 4),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.cnn_4 = nn.Sequential(\n",
    "            nn.Conv2d(self.hidden_dim * 4, 1, kernel_size=(4,4), stride=(1,1), padding=(0,0), bias=False)\n",
    "        )\n",
    "\n",
    "        # self.sparse_cnn_4 = nn.Sequential(\n",
    "        #     nn.Conv2d(self.hidden_dim * 4, self.input_dim + self.num_of_classes, kernel_size=(4,4), stride=(1,1), padding=(0,0), bias=False)\n",
    "        # )\n",
    "\n",
    "        # self.fc_1 = nn.Linear(2560, int(self.hidden_dim))\n",
    "        # self.fc_2 = nn.Linear(int(self.hidden_dim), int(self.hidden_dim))\n",
    "        # self.fc_out  = nn.Linear(int(self.hidden_dim),self.batch_size)\n",
    "        #\n",
    "        # self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "        self.weights_init()\n",
    "    def weights_init(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "            torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "        if isinstance(m, nn.BatchNorm2d):\n",
    "            torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "            torch.nn.init.constant_(m.bias, val=0)\n",
    "    def forward(self, x, y):\n",
    "        #print(\"y\",y.size())\n",
    "        #print(\"x\",x.size())\n",
    "\n",
    "        #print(\"0\",x.size())\n",
    "        x = self.cnn_1(x)\n",
    "        #print(\"1\",x.size())\n",
    "        x = self.cnn_2(x)\n",
    "        x = self.cnn_3(x)\n",
    "        #print(\"2\",x.size())\n",
    "        x = self.cnn_4(x)\n",
    "        #print(\"3\",x.size())# 3 torch.Size([512, 4, 1, 1])\n",
    "        #print(\"y\",y.size())# y torch.Size([512, 1])\n",
    "        #y = torch.reshape(y,(self.batch_size,1,1,1))\n",
    "        #print(\"yr\",y.size())\n",
    "        # x = torch.reshape(x,(512,4))\n",
    "        #\n",
    "        # #print(\"xf\",x.size())\n",
    "        # x = torch.cat([x,y], 1)\n",
    "        # x = torch.flatten(x)\n",
    "        #\n",
    "        #\n",
    "        # #print(\"4\",x.size()) # 4 torch.Size([2560])\n",
    "        # x = self.LeakyReLU(self.fc_1(x))\n",
    "        # #print(\"5\",x.size())\n",
    "        # x = self.LeakyReLU(self.fc_2(x))\n",
    "        # #print(\"6\",x.size())\n",
    "        # x = self.fc_out(x)\n",
    "        #print(\"4\",x.size())\n",
    "        return torch.sigmoid(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cond_generator = Cond_Generator(batch_size=batch_size,latent_dim=latent_dim, hidden_dim=128, output_dim=img_size, num_of_classes=len(dataset.classes),SPARSITY_CNN=SPARSITY_CNN,BOOST_STRENGTH=BOOST_STRENGTH,PERCENT_ON=PERCENT_ON,relu_flag=relu_flag).to(device)\n",
    "cond_discriminator = Cond_Discriminator( hidden_dim=32, input_dim=img_size, num_of_classes=len(dataset.classes),batch_size=batch_size,SPARSITY_CNN=SPARSITY_CNN,BOOST_STRENGTH=BOOST_STRENGTH,PERCENT_ON=PERCENT_ON,relu_flag=relu_flag).to(device)\n",
    "\n",
    "# Optimizers\n",
    "generator_optimizer = torch.optim.Adam(cond_generator.parameters(), lr=0.0001)\n",
    "# generator_optimizer = torch.optim.RMSprop(cond_generator.parameters(), lr=0.00005)\n",
    "generator_scheduler = optim.lr_scheduler.ExponentialLR(optimizer=generator_optimizer, gamma=0.99)\n",
    "discriminator_optimizer = torch.optim.Adam(cond_discriminator.parameters(), lr=0.0001)\n",
    "# discriminator_optimizer = torch.optim.RMSprop(cond_discriminator.parameters(), lr=0.00005)\n",
    "discriminator_scheduler = optim.lr_scheduler.ExponentialLR(optimizer=discriminator_optimizer, gamma=0.99)\n",
    "\n",
    "# loss\n",
    "#criterion = nn.MSELoss()\n",
    "criterion  = nn.BCELoss()\n",
    "#criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = torch.randn(32, latent_dim,device=device)\n",
    "fixed_labels = torch.randint(len(dataset.classes),(32,),device=device)\n",
    "fixed_labels = F.one_hot(fixed_labels, len(dataset.classes)).float()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Parametres - G :  3370686\n",
      "No. of Trainable parametres - G :  3370686\n",
      "No. of Parametres - D :  167136\n",
      "No. of Trainable parametres - D :  167136\n"
     ]
    }
   ],
   "source": [
    "G_losses = []\n",
    "D_losses = []\n",
    "\n",
    "################# NO. PARAMS ####################\n",
    "G_total_params = sum(p.numel() for p in cond_generator.parameters() if p.requires_grad)\n",
    "print(\"No. of Parametres - G : \", G_total_params)\n",
    "G_total_params = sum(p.numel() for p in cond_generator.parameters() if p.requires_grad)\n",
    "print(\"No. of Trainable parametres - G : \", G_total_params)\n",
    "\n",
    "D_total_params = sum(p.numel() for p in cond_discriminator.parameters() if p.requires_grad)\n",
    "print(\"No. of Parametres - D : \", D_total_params)\n",
    "D_total_params = sum(p.numel() for p in cond_discriminator.parameters() if p.requires_grad)\n",
    "print(\"No. of Trainable parametres - D : \", D_total_params)\n",
    "################# NO. PARAMS ####################\n",
    "\n",
    "num_epochs = 40\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in the dataloader\n",
    "    discriminator_fake_acc = []\n",
    "    discriminator_real_acc = []\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        discriminator_optimizer.zero_grad()\n",
    "        # Format batch\n",
    "        real_images = data[0].to(device)\n",
    "        b_size = real_images.size(0)\n",
    "        #for _ in range(disc_iter):\n",
    "        y = data[1]\n",
    "        y = F.one_hot(y, num_classes=len(dataset.classes)).to(device).float()\n",
    "\n",
    "        label = torch.full((b_size,),0.9, dtype=torch.float, device=device) # Setting labels for real images & preventing overconfidence\n",
    "        # Forward pass real batch through D\n",
    "        output = cond_discriminator(real_images, y).view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        error_discriminator_real = criterion(output, label)\n",
    "        #error_discriminator_real = torch.mean(output)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        error_discriminator_real.backward()\n",
    "        discriminator_real_acc.append(output.mean().item())\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(b_size, latent_dim,device=device)\n",
    "        rand_y = torch.randint(len(dataset.classes),(b_size,),device=device)\n",
    "        rand_y = F.one_hot(rand_y, len(dataset.classes)).float()\n",
    "        # Generate fake image batch with Generator\n",
    "        fake_images = cond_generator(noise, rand_y)\n",
    "        label_fake = torch.full((b_size,),0.00, dtype=torch.float, device=device)\n",
    "        # Classify all fake batch with Discriminator\n",
    "        output = cond_discriminator(fake_images.detach(), rand_y).view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        error_discriminator_fake = criterion(output, label_fake)\n",
    "        #error_discriminator_fake = torch.mean(output)\n",
    "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "        error_discriminator_fake.backward()\n",
    "        discriminator_fake_acc.append(output.mean().item())\n",
    "        # Compute error of D as sum over the fake and the real batches\n",
    "        error_discriminator = error_discriminator_real + error_discriminator_fake\n",
    "        # Update D\n",
    "        #error_discriminator.backward()\n",
    "        discriminator_optimizer.step()\n",
    "\n",
    "        # for p in cond_discriminator.parameters():\n",
    "        #         p.data.clamp_(-WEIGHT_CLIP, WEIGHT_CLIP)\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "#         for _ in range(3):\n",
    "        noise = torch.randn(b_size, latent_dim,device=device)\n",
    "        rand_y = torch.randint(len(dataset.classes),(b_size,),device=device)\n",
    "        rand_y = F.one_hot(rand_y, len(dataset.classes)).float()\n",
    "        fake_images = cond_generator(noise, rand_y)\n",
    "        generator_optimizer.zero_grad()\n",
    "        label = torch.full((b_size,),1., dtype=torch.float, device=device)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output = cond_discriminator(fake_images, rand_y).view(-1)\n",
    "        # Calculate G's loss based on this output\n",
    "        error_generator = criterion(output, label)\n",
    "        #error_generator = torch.mean(output)\n",
    "        # Calculate gradients for G\n",
    "        error_generator.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        generator_optimizer.step()\n",
    "\n",
    "        # Output training stats\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(error_generator.item())\n",
    "        D_losses.append(error_discriminator.item())\n",
    "\n",
    "\n",
    "    print(f\"Epoch: {epoch}, discrimiantor fake error: {np.mean(discriminator_fake_acc):.3}, discriminator real acc: {np.mean(discriminator_real_acc):.3}\")\n",
    "    generator_scheduler.step()\n",
    "    discriminator_scheduler.step()\n",
    "    cond_generator.apply(update_boost_strength)\n",
    "    cond_discriminator.apply(update_boost_strength)\n",
    "    if epoch % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            fake = cond_generator(fixed_noise, fixed_labels).detach().cpu()\n",
    "        grid = torchvision.utils.make_grid(fake,nrow=16, normalize=True)\n",
    "        grid = grid.permute(1, 2, 0)\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.title(f\"Generations\")\n",
    "        plt.imshow(grid)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generowanie wynik√≥w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(number_of_images, clazz=None):\n",
    "    fixed_noise = torch.randn(number_of_images, latent_dim, device=device)\n",
    "    fixed_labels = torch.randint(len(dataset.classes), (number_of_images,), device=device)\n",
    "    if clazz:\n",
    "        fixed_labels = torch.tensor([clazz for _ in range(number_of_images)], device=device)\n",
    "    fixed_labels = F.one_hot(fixed_labels, len(dataset.classes)).float()\n",
    "    with torch.no_grad():\n",
    "        fake = cond_generator(fixed_noise, fixed_labels).detach().cpu()\n",
    "        grid = torchvision.utils.make_grid(fake)\n",
    "        grid = grid.permute(1, 2, 0)\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.title(f\"Generations\")\n",
    "        plt.imshow(grid)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_images(number_of_images=8, clazz=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SSNE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08f109b28ac4d856f241b97928d0d5bfddfa33a99d55412376cd8c61c424e303"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}